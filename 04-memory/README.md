# LLM の記憶と情報検索

これまではシングルターンと呼ばれる、一度きりの応答が中心でした。

AI エージェントは基本的にマルチターン、会話を記憶しながらやりとりしていく方法や  
「思い出す」「より正確な情報に基づき会話する」ための RAG 技術をみていきます。

## 用語の復習

- セッション (Session): 1 度の会話。今いい？から、ありがとう、またね！まで。
- 短期記憶 (Short-term Memory): セッション冒頭からの記憶。全部憶えた状態で会話をする。
- 長期記憶 (Long-term Memory): その相手との過去のやり取りすべて。大事なことは憶えている。
- 埋め込み (Embeddings): 情報を高次元空間のベクトルに変換する技術、もしくはその値。
- コサイン類似度: ベクトル同士がいかに近しいかを測るための指標。
- グラフ: 項目感の関係性をデータとしてもつための概念。

## 1. 会話履歴

シンプルに、セッションでの会話履歴をすべてローカル変数に保持し  
都度 LLM にコンテキストごと渡す実装から見てみましょう。

```bash
python 04-memory/01-history.py
```

## 2. 会話の要点のみを保存

長期記憶としてすべての情報を維持、LLM に渡すのは無理があります。  
そこで LLM に会話をまとめさせ、それを保持するのが現実的です。

以下は単純にまとめを作ってくれるライブラリによる実装例です。  
会話のコンテキストをサマリーとして抽出し、会話に組み込めます。

```bash
python 04-memory/02-summarized-history.py
```

## 3. 長期記憶からの検索

保存された記憶から、思い出すべきことだけを思い出すにはどうすればいいでしょうか。

ベクトル検索と、知識グラフを使った実装を試していきます。  
ここではベクトル検索の基礎となる、文章の埋め込みから確認します。

名古屋大学 笹野研究室の[日本語対応 Embedding モデル](https://huggingface.co/cl-nagoya)から  
[パラメタ数 37M の Ruri v3](https://huggingface.co/cl-nagoya/ruri-v3-30m) を使います。  
文章が 256 次元のベクトルになる様子や、意味合いが近い様子、  
DB からの検索結果などを確認してみてください。

```bash
python 04-memory/03-embeddings.py
```

## 4. RAG

実際にドキュメントを RAG で扱ってみましょう。  
基本的な手順は・・

1. ドキュメントは事前に `チャンク` に区切りってベクトル化、保存しておきます
2. LLM はユーザから問い合わせを受けます（ex. うちの会社に慶弔見舞金ってある？あるならどう申請すればいいの？）
3. LLM はベクトル DB への検索ワードを考え、クエリします
4. ベクトル DB は `関連度合いの高い` チャンクを複数応答します
5. LLM は複数のチャンクを念頭においてユーザに応答します

`1.` は本来、事前 / 非同期にやっておく必要がありますが  
どういう頻度で必要なのか、洗い替えか、追記でいいのか  
データへのアクセス権はどう制御すべきかなども考慮が必要です。

またベクトル検索の精度・速度といった性能には

- どういう粒度でチャンキングするか
- Embedding モデルに何を使うか
- Annoy や ScaNN など検索アルゴリズムには何を使うか
- データベースに何を使うか
- リランク・フィルタを実装するか、何を使うか

といったことも関わってきて沼です。  
わたしはマネージドサービスを使いたい。

```bash
python 04-memory/04-rag.py
```

## 5. トーカナイザー

いったん脱線します。

ドキュメントは `チャンク` に分けてベクトル化していました。  
検索は精度とメモリ効率のバランスから、ユースケースにはよれど  
256 ~ 1,024 次元前後が選ばれる傾向にあるようです。

一方 LLM 学習でも Embedding モデルはでてきますよね。  
そちらは数千から万を超える次元とも言われます。  
また、LLM ではチャンクではなく `トークン` 単位でベクトル化します。  
その様子も確認しておきましょう！

```bash
python 04-memory/05-tokenizer.py
```

## 6. グラフ RAG

RAG にもどります。

データの持ち方として、ベクトルではなくナレッジグラフを活用する方法もあります。  
事前の情報を整理する段階で、関係性を明確にしておくことで  
あとからチャンクの不足や解釈を間違えるといったリスクを低減できます。

```bash
python 04-memory/06-graph-rag.py
```

Neo4j に格納されたデータを見てみましょう！  
以下のリンクを開き、ノードとリレーションシップが正しそうかを確認してみてください。

http://localhost:7474/browser/?cmd=edit&arg=match%20(n)%20return%20n
